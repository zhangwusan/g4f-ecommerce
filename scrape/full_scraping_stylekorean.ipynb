{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40744dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all categories from the website base on the categories list\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Disable SSL warnings\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d68a13",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a02e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subcategories_to_file(sub_categories: List[Dict], filename=\"sub_categories.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sub_categories, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Subcategories saved to {filename}\")\n",
    "\n",
    "def read_subcategories_from_file(filename=\"sub_categories.json\") -> List[Dict]:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def read_all_url_from_file(path: str) -> List[Dict]:\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    return re.sub(r'[^\\w\\-_.]', '_', name)\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def get_logger(name, log_path):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    handler = logging.FileHandler(log_path, mode='a', encoding='utf-8')\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "def load_fetched_urls_from_log(log_file):\n",
    "    urls = set()\n",
    "    if not os.path.exists(log_file):\n",
    "        return urls\n",
    "\n",
    "    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Look for lines like: \"YYYY-MM-DD HH:MM:SS,mmm - INFO - Fetched: https://example.com/product/123\"\n",
    "            if \"Fetched:\" in line:\n",
    "                parts = line.split(\"Fetched:\")\n",
    "                if len(parts) == 2:\n",
    "                    url = parts[1].strip()\n",
    "                    urls.add(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227e280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subcategories(\n",
    "    url: str, \n",
    "    categories_table_header_class: str = \"brand_cat_table\",\n",
    "    domain: str = \"https://www.stylekorean.com\"\n",
    ") -> List[Dict]:\n",
    "    response = requests.get(url, headers=headers, verify=False)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", class_=categories_table_header_class)\n",
    "    if not table:\n",
    "        print(f\"No table found with class '{categories_table_header_class}'\")\n",
    "        return []\n",
    "\n",
    "    trs = table.find_all(\"tr\")\n",
    "    sub_categories = []\n",
    "\n",
    "    for tr in trs:\n",
    "        th = tr.find(\"th\")\n",
    "        td = tr.find(\"td\")\n",
    "        \n",
    "        if th and td:\n",
    "            parent_link = th.find(\"a\")\n",
    "            if not parent_link:\n",
    "                continue\n",
    "\n",
    "            parent_name = parent_link.text.strip()\n",
    "            parent_url = domain + parent_link['href']\n",
    "\n",
    "            children = []\n",
    "            for link in td.find_all(\"a\"):\n",
    "                child_name = link.text.strip()\n",
    "                child_url = domain + link['href']\n",
    "                children.append({\n",
    "                    \"name\": child_name,\n",
    "                    \"url\": child_url\n",
    "                })\n",
    "\n",
    "            sub_categories.append({\n",
    "                \"name\": parent_name,\n",
    "                \"url\": parent_url,\n",
    "                \"children\": children\n",
    "            })\n",
    "\n",
    "    return sub_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758ed345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagination_info(soup: BeautifulSoup) -> Tuple[int, int]:\n",
    "    pagination_nav_class = \"pg_wrap\"\n",
    "    pagination_current_strong_class = \"pg_current\"\n",
    "    pagination_last_link_class = \"pg_page pg_end\"\n",
    "    pagination = soup.find(\"nav\", class_=pagination_nav_class)\n",
    "    \n",
    "    if not pagination:\n",
    "        print(f\"[!] No pagination found with class '{pagination_nav_class}'\")\n",
    "        return 1, 1\n",
    "\n",
    "    # Current page\n",
    "    current_page_tag = pagination.find(\"strong\", class_=pagination_current_strong_class)\n",
    "    current_page_number = int(current_page_tag.text.strip()) if current_page_tag else 1\n",
    "\n",
    "    # Last page\n",
    "    last_page_tag = pagination.find(\"a\", class_=pagination_last_link_class)\n",
    "\n",
    "    if last_page_tag and 'href' in last_page_tag.attrs:\n",
    "        match = re.search(r'page=(\\d+)', last_page_tag['href'])\n",
    "        last_page_number = int(match.group(1)) if match else current_page_number\n",
    "    else:\n",
    "        last_page_number = current_page_number\n",
    "\n",
    "    return current_page_number, last_page_number\n",
    "\n",
    "\n",
    "def scrape_all_products_from_paginated_category(base_url: str, headers: dict, delay: float = 1.0) -> List[Dict[str, str]]:\n",
    "    list_products = []\n",
    "    silder_class = \"productlist_skin\"\n",
    "    item_class = \"sct_li\"\n",
    "    product_name_class = \"sct_txt\"\n",
    "\n",
    "    print(f\"Fetching base URL: {base_url}\")\n",
    "    response = requests.get(base_url, headers=headers, verify=False)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    current_page, last_page = get_pagination_info(soup)\n",
    "    print(f\"Current Page: {current_page}, Last Page: {last_page}\")\n",
    "\n",
    "    for page in range(1, last_page + 1):\n",
    "        print(f\"\\nScraping page {page} of {last_page}...\")\n",
    "        paged_url = f\"{base_url}&page={page}\"\n",
    "        response = requests.get(paged_url, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        grid = soup.find(\"div\", class_=silder_class)\n",
    "        if not grid:\n",
    "            print(f\"[!] No product grid found on page {page}\")\n",
    "            continue\n",
    "\n",
    "        items = grid.find_all(\"li\", class_=item_class)\n",
    "        if not items:\n",
    "            print(f\"[!] No items found in grid on page {page}\")\n",
    "            continue\n",
    "\n",
    "        for item in items:\n",
    "            product = item.find(\"p\", class_=product_name_class)\n",
    "            if not product:\n",
    "                continue\n",
    "            link = product.find(\"a\")\n",
    "            if not link:\n",
    "                continue\n",
    "            product_name = link.text.strip()\n",
    "            product_url = link['href']\n",
    "            list_products.append({\n",
    "                \"name\": product_name,\n",
    "                \"url\": product_url\n",
    "            })\n",
    "\n",
    "        # Sleep before requesting the next page\n",
    "        print(f\"Sleeping for {delay} seconds before next page...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return list_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa7e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_data(soup: BeautifulSoup, domain: str = \"\") -> dict:\n",
    "    title_h1 = soup.find(\"h1\", id=\"sit_title\")\n",
    "    full_text = title_h1.text.strip() if title_h1 and title_h1.text else \"No Title Found\"\n",
    "    brand_link = title_h1.find(\"a\") if title_h1 else None\n",
    "    brand_name = brand_link.text.strip() if brand_link and brand_link.text else \"No Brand Name Found\"\n",
    "    product_name = full_text.replace(brand_name, '').strip()\n",
    "\n",
    "    # Price and discount\n",
    "    table = soup.find(\"table\", class_=\"sit_ov_tbl\")\n",
    "    td = table.find_all(\"tr\") if table else []\n",
    "    span = td[2].find_all('span') if len(td) > 2 else []\n",
    "    price_raw = span[0].text.strip() if len(span) > 0 and span[0].text else \"0\"\n",
    "    discount_raw = span[1].text.strip() if len(span) > 1 and span[1].text else \"0%\"\n",
    "    discount_digits = re.sub(r\"[^\\d]\", \"\", discount_raw)\n",
    "    discount = int(discount_digits) if discount_digits else 0\n",
    "    price_match = re.search(r\"\\d+(\\.\\d+)?\", price_raw)\n",
    "    discount_price = float(price_match.group()) if price_match else 0.0\n",
    "    price = round(discount_price / (1 - discount / 100), 2) if discount else discount_price\n",
    "\n",
    "    # Images\n",
    "    gallary_div = soup.find(\"div\", id=\"sit_pvi_big\")\n",
    "    sit_pvi_big = gallary_div.find_all(\"a\") if gallary_div else []\n",
    "    images = []\n",
    "    for link in sit_pvi_big:\n",
    "        img_tag = link.find(\"img\") if link else None\n",
    "        if img_tag and img_tag.has_attr(\"src\"):\n",
    "            images.append({\"url\": img_tag[\"src\"]})\n",
    "\n",
    "    # Product info (code & weight)\n",
    "    product_info = soup.find(\"div\", class_=\"pro_info\")\n",
    "    info_text = product_info.text.replace(\"\\xa0\", \" \").replace(\"\\n\", \" \").strip() if product_info and product_info.text else \"\"\n",
    "    \n",
    "    code_match = re.search(r\"Code\\s*:\\s*([A-Z0-9\\-]+)\", info_text)\n",
    "    weight_match = re.search(r\"Weight\\s*:\\s*([^\\n\\r]+)\", info_text)\n",
    "    code = code_match.group(1).strip() if code_match else None\n",
    "    weight = weight_match.group(1).strip() if weight_match else None\n",
    "\n",
    "    # Notifications\n",
    "    notifications = []\n",
    "    sub_noti = soup.find(\"div\", class_=\"sub_noti\")\n",
    "    sub_noti_inners = sub_noti.find_all(\"div\", class_=\"sub_noti_inner\") if sub_noti else []\n",
    "    for inner in sub_noti_inners:\n",
    "        title = inner.find(\"div\", class_=\"sub_noti_title\")\n",
    "        content = inner.find(\"div\", class_=\"sub_noti_cont\")\n",
    "        notifications.append({\n",
    "            \"title\": title.text.strip() if title and title.text else \"No Title\",\n",
    "            \"content\": content.text.strip() if content and content.text else \"No Content\"\n",
    "        })\n",
    "\n",
    "    # Descriptions and usage\n",
    "    main_taps2 = soup.find(\"div\", id=\"main_taps2\")\n",
    "    yk0 = main_taps2.find(\"div\", id=\"main_tabcontent_yk0\") if main_taps2 else None\n",
    "    yk1 = main_taps2.find(\"div\", id=\"main_tabcontent_yk1\") if main_taps2 else None\n",
    "\n",
    "    desc_div = yk0.find(\"div\", class_=\"is_contents\") if yk0 else None\n",
    "    description = desc_div.text.strip() if desc_div and desc_div.text else \"No Description Found\"\n",
    "\n",
    "    ps = yk1.find_all(\"p\") if yk1 else []\n",
    "    extra_description = ps[0].text.strip() if len(ps) > 0 and ps[0].text else \"No Extra Description Found\"\n",
    "    if len(ps) > 1:\n",
    "        img_tag = ps[1].find(\"img\")\n",
    "        instruction_image = img_tag[\"src\"] if img_tag and img_tag.has_attr(\"src\") else \"No Instruction Image Found\"\n",
    "    else:\n",
    "        instruction_image = \"No Instruction Image Found\"\n",
    "\n",
    "    # Shipping Info\n",
    "    shiping_info = soup.find(\"div\", id=\"shipping\")\n",
    "    shipping_text = shiping_info.text.strip() if shiping_info and shiping_info.text else \"No Shipping Info Found\"\n",
    "\n",
    "    # Reviews\n",
    "    review_table = soup.find(\"table\", class_=\"review_table\")\n",
    "    tds = review_table.find_all(\"td\") if review_table else []\n",
    "    reviews = []\n",
    "    i = 0\n",
    "    while i < len(tds):\n",
    "        if tds[i].has_attr(\"colspan\"):\n",
    "            i += 1\n",
    "            continue\n",
    "        try:\n",
    "            star_img_tag = tds[i].find(\"img\") if i < len(tds) else None\n",
    "            star_img = star_img_tag[\"alt\"] if star_img_tag and star_img_tag.has_attr(\"alt\") else None\n",
    "            title = tds[i + 1].text.strip() if i + 1 < len(tds) and tds[i + 1] else \"No Title\"\n",
    "            email = tds[i + 2].text.strip() if i + 2 < len(tds) and tds[i + 2] else \"No Email\"\n",
    "            content = tds[i + 3].decode_contents().strip() if i + 3 < len(tds) and tds[i + 3] else \"No Content\"\n",
    "            date_text = tds[i + 5].text.strip() if i + 5 < len(tds) and tds[i + 5] else \"\"\n",
    "            date_match = re.search(r\"Posted on (\\d{2}-\\d{2}-\\d{2})\", date_text)\n",
    "            date = date_match.group(1) if date_match else None\n",
    "\n",
    "            reviews.append({\n",
    "                \"stars\": star_img,\n",
    "                \"title\": title,\n",
    "                \"email\": email,\n",
    "                \"content\": content,\n",
    "                \"date\": date,\n",
    "            })\n",
    "            i += 7\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping review at index {i} due to error: {e}\")\n",
    "            i += 1\n",
    "\n",
    "    return {\n",
    "        \"code\": code,\n",
    "        \"weight\": weight,\n",
    "        \"product_name\": product_name,\n",
    "        \"brand\": brand_name.strip('[]'),\n",
    "        \"brand_url\": f\"{domain}{brand_link['href']}\" if brand_link and brand_link.has_attr(\"href\") else \"No URL Found\",\n",
    "        \"price\": price,\n",
    "        \"discount\": discount,\n",
    "        \"discount_price\": discount_price,\n",
    "        \"images\": images,\n",
    "        \"notifications\": notifications,\n",
    "        \"description\": description,\n",
    "        \"extra_description\": extra_description,\n",
    "        \"instruction_image\": instruction_image,\n",
    "        \"shipping_info\": shipping_text,\n",
    "        \"reviews\": reviews\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6dde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_NAME = \"stylekorean\"\n",
    "SUB_CATEGORIES = \"sub_categories\"\n",
    "ALL_PRODUCT_URLS = \"product_urls\"\n",
    "DETAIL_PRODUCTS = \"product_details\"\n",
    "DOMAIN = \"https://www.stylekorean.com\"\n",
    "LOG_DIR = \"log\"\n",
    "DATA_DIR = \"data\"\n",
    "DELAY_BETWEEN_FILES = 1\n",
    "\n",
    "HEADER = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "CATEGORIES = [\n",
    "    {\"Skincare\": f\"{DOMAIN}/shop/list.php?ca_id=10\"},\n",
    "    {\"Makeup\": f\"{DOMAIN}/shop/list.php?ca_id=11\"},\n",
    "    {\"Body Care\": f\"{DOMAIN}/shop/list.php?ca_id=12\"},\n",
    "    {\"Hair Care\": f\"{DOMAIN}/shop/list.php?ca_id=13\"},\n",
    "    {\"Beautiful Device & Tools\": f\"{DOMAIN}/shop/list.php?ca_id=14\"}\n",
    "]\n",
    "\n",
    "# Directories\n",
    "subcat_data_dir = os.path.join(DATA_NAME, DATA_DIR, SUB_CATEGORIES)\n",
    "product_data_root = os.path.join(DATA_NAME, DATA_DIR, ALL_PRODUCT_URLS)\n",
    "product_detail_data_dir = os.path.join(DATA_NAME, DATA_DIR, DETAIL_PRODUCTS)\n",
    "\n",
    "subcat_log_dir = os.path.join(DATA_NAME, LOG_DIR, SUB_CATEGORIES)\n",
    "product_log_dir = os.path.join(DATA_NAME, LOG_DIR, ALL_PRODUCT_URLS)\n",
    "product_detail_log_dir = os.path.join(DATA_NAME, LOG_DIR, DETAIL_PRODUCTS)\n",
    "\n",
    "os.makedirs(subcat_data_dir, exist_ok=True)\n",
    "os.makedirs(product_data_root, exist_ok=True)\n",
    "os.makedirs(product_detail_data_dir, exist_ok=True)\n",
    "\n",
    "os.makedirs(subcat_log_dir, exist_ok=True)\n",
    "os.makedirs(product_log_dir, exist_ok=True)\n",
    "os.makedirs(product_detail_log_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8345d4",
   "metadata": {},
   "source": [
    "# Get Link for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d4ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main Categories: 100%|██████████| 5/5 [00:00<00:00, 951.82category/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Skincare → Already scraped and saved: stylekorean/data/sub_categories/skincare_sub_categories.json\n",
      "[SKIP] Makeup → Already scraped and saved: stylekorean/data/sub_categories/makeup_sub_categories.json\n",
      "[SKIP] Body Care → Already scraped and saved: stylekorean/data/sub_categories/body_care_sub_categories.json\n",
      "[SKIP] Hair Care → Already scraped and saved: stylekorean/data/sub_categories/hair_care_sub_categories.json\n",
      "[SKIP] Beautiful Device & Tools → Already scraped and saved: stylekorean/data/sub_categories/beautiful_device_&_tools_sub_categories.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for category in tqdm(CATEGORIES, desc=\"Main Categories\", unit=\"category\"):\n",
    "    for name, url in category.items():\n",
    "        sanitized_name = name.replace(' ', '_').lower()\n",
    "        log_path = os.path.join(subcat_log_dir, f\"{sanitized_name}.log\")\n",
    "        logger = get_logger(name, log_path)\n",
    "\n",
    "        filename = f\"{sanitized_name}_sub_categories.json\"\n",
    "        filepath = os.path.join(subcat_data_dir, filename)\n",
    "\n",
    "        # Recovery check\n",
    "        if os.path.exists(filepath):\n",
    "            msg = f\"[SKIP] {name} → Already scraped and saved: {filepath}\"\n",
    "            logger.info(msg)\n",
    "            tqdm.write(msg)\n",
    "            continue\n",
    "\n",
    "        msg = f\"[START] Scraping subcategories for {name} → {url}\"\n",
    "        logger.info(msg)\n",
    "        tqdm.write(msg)\n",
    "\n",
    "        try:\n",
    "            sub_categories = scrape_subcategories(url)\n",
    "            if sub_categories:\n",
    "                save_subcategories_to_file(sub_categories=sub_categories, filename=filepath)\n",
    "                msg = f\"[DONE] {name} → Saved {len(sub_categories)} subcategories to {filepath}\"\n",
    "                logger.info(msg)\n",
    "                tqdm.write(msg)\n",
    "            else:\n",
    "                msg = f\"[WARN] {name} → No subcategories found at {url}\"\n",
    "                logger.warning(msg)\n",
    "                tqdm.write(msg)\n",
    "        except Exception as e:\n",
    "            msg = f\"[ERROR] {name} → Failed to scrape subcategories: {str(e)}\"\n",
    "            logger.error(msg)\n",
    "            tqdm.write(msg)\n",
    "\n",
    "        time.sleep(DELAY_BETWEEN_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed59ce",
   "metadata": {},
   "source": [
    "# Read data from previous file to get all product urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f058a1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: stylekorean/log/product_urls/skincare_sub_categories.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      \n",
      "Processing Subcategory Files:   0%|          | 0/5 [00:00<?, ?file/s]                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Sheet Masks (270) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Sheet_Masks__270_.json\n",
      "[SKIP] Sleeping Masks (23) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Sleeping_Masks__23_.json\n",
      "[SKIP] Pad (96) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Pad__96_.json\n",
      "[SKIP] Patch (68) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Patch__68_.json\n",
      "[SKIP] Wash off Masks (82) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Wash_off_Masks__82_.json\n",
      "[SKIP] Nose Pack (4) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Nose_Pack__4_.json\n",
      "[SKIP] Toner (213) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Toner__213_.json\n",
      "[SKIP] Emulsion (47) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Emulsion__47_.json\n",
      "[SKIP] Essence & Serum (563) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Essence___Serum__563_.json\n",
      "[SKIP] Cream (405) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Cream__405_.json\n",
      "[SKIP] Face Mist & Fixer (26) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Face_Mist___Fixer__26_.json\n",
      "[SKIP] Oil (7) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Oil__7_.json\n",
      "[SKIP] Eye Cream (65) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Eye_Cream__65_.json\n",
      "[SKIP] Special Gift Sets (56) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Special_Gift_Sets__56_.json\n",
      "[SKIP] For Men (4) — already exists: stylekorean/data/product_urls/skincare_sub_categories/For_Men__4_.json\n",
      "[SKIP] Cleansing Foam (214) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Cleansing_Foam__214_.json\n",
      "[SKIP] Cleansing Oil & Water (76) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Cleansing_Oil___Water__76_.json\n",
      "[SKIP] Cleansing balm (35) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Cleansing_balm__35_.json\n",
      "[SKIP] Exfoliators (24) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Exfoliators__24_.json\n",
      "[SKIP] Makeup Remover (6) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Makeup_Remover__6_.json\n",
      "[SKIP] Sun Creams & Fluids (196) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Sun_Creams___Fluids__196_.json\n",
      "[SKIP] Sun Sticks & Cushions (37) — already exists: stylekorean/data/product_urls/skincare_sub_categories/Sun_Sticks___Cushions__37_.json\n",
      "[COMPLETE] Finished scraping: skincare_sub_categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: stylekorean/log/product_urls/makeup_sub_categories.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            \n",
      "Processing Subcategory Files:  20%|██        | 1/5 [00:01<00:04,  1.05s/file]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Foundation (27) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Foundation__27_.json\n",
      "[SKIP] Makeup Base (21) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Makeup_Base__21_.json\n",
      "[SKIP] Powder & Pact (13) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Powder___Pact__13_.json\n",
      "[SKIP] Concealer (10) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Concealer__10_.json\n",
      "[SKIP] Cushion (64) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Cushion__64_.json\n",
      "[SKIP] Blusher & Highlighter (43) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Blusher___Highlighter__43_.json\n",
      "[SKIP] Eyebrows (13) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Eyebrows__13_.json\n",
      "[SKIP] Eyeshadow (40) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Eyeshadow__40_.json\n",
      "[SKIP] Eyeliner (11) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Eyeliner__11_.json\n",
      "[SKIP] Mascara (18) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Mascara__18_.json\n",
      "[SKIP] Lip Stick (29) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Lip_Stick__29_.json\n",
      "[SKIP] Lip Tint (88) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Lip_Tint__88_.json\n",
      "[SKIP] Lip Gloss (25) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Lip_Gloss__25_.json\n",
      "[SKIP] Lip Care (82) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Lip_Care__82_.json\n",
      "[SKIP] Nails (5) — already exists: stylekorean/data/product_urls/makeup_sub_categories/Nails__5_.json\n",
      "[COMPLETE] Finished scraping: makeup_sub_categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subcategory Files:  40%|████      | 2/5 [00:02<00:03,  1.05s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: stylekorean/log/product_urls/body_care_sub_categories.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Subcategory Files:  40%|████      | 2/5 [00:02<00:03,  1.05s/file]              \n",
      "Processing Subcategory Files:  40%|████      | 2/5 [00:02<00:03,  1.05s/file]              \n",
      "Processing Subcategory Files:  40%|████      | 2/5 [00:02<00:03,  1.05s/file]              \n",
      "Processing Subcategory Files:  40%|████      | 2/5 [00:02<00:03,  1.05s/file]              \n",
      "Processing Subcategory Files:  40%|████      | 2/5 [00:02<00:03,  1.05s/file]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Body Lotion & Oils (44) — already exists: stylekorean/data/product_urls/body_care_sub_categories/Body_Lotion___Oils__44_.json\n",
      "[SKIP] Body wash (28) — already exists: stylekorean/data/product_urls/body_care_sub_categories/Body_wash__28_.json\n",
      "[SKIP] Body Scrub (2) — already exists: stylekorean/data/product_urls/body_care_sub_categories/Body_Scrub__2_.json\n",
      "[SKIP] Hand & Foot & Oral Care (33) — already exists: stylekorean/data/product_urls/body_care_sub_categories/Hand___Foot___Oral_Care__33_.json\n",
      "[SKIP] Body Mist & Fragrance (11) — already exists: stylekorean/data/product_urls/body_care_sub_categories/Body_Mist___Fragrance__11_.json\n",
      "[COMPLETE] Finished scraping: body_care_sub_categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subcategory Files:  60%|██████    | 3/5 [00:03<00:02,  1.04s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: stylekorean/log/product_urls/hair_care_sub_categories.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Subcategory Files:  60%|██████    | 3/5 [00:03<00:02,  1.04s/file]              \n",
      "Processing Subcategory Files:  60%|██████    | 3/5 [00:03<00:02,  1.04s/file]              \n",
      "Processing Subcategory Files:  60%|██████    | 3/5 [00:03<00:02,  1.04s/file]              \n",
      "Processing Subcategory Files:  60%|██████    | 3/5 [00:03<00:02,  1.04s/file]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Shampoo & Conditioner (40) — already exists: stylekorean/data/product_urls/hair_care_sub_categories/Shampoo___Conditioner__40_.json\n",
      "[SKIP] Hair Treatment (25) — already exists: stylekorean/data/product_urls/hair_care_sub_categories/Hair_Treatment__25_.json\n",
      "[SKIP] Hair Essence & Serum (18) — already exists: stylekorean/data/product_urls/hair_care_sub_categories/Hair_Essence___Serum__18_.json\n",
      "[SKIP] Hair Color & Styling (8) — already exists: stylekorean/data/product_urls/hair_care_sub_categories/Hair_Color___Styling__8_.json\n",
      "[COMPLETE] Finished scraping: hair_care_sub_categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subcategory Files:  80%|████████  | 4/5 [00:04<00:01,  1.03s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: stylekorean/log/product_urls/beautiful_device_&_tools_sub_categories.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Subcategory Files:  80%|████████  | 4/5 [00:04<00:01,  1.03s/file]                             \n",
      "Processing Subcategory Files:  80%|████████  | 4/5 [00:04<00:01,  1.03s/file]                             \n",
      "Processing Subcategory Files:  80%|████████  | 4/5 [00:04<00:01,  1.03s/file]                             \n",
      "Processing Subcategory Files:  80%|████████  | 4/5 [00:04<00:01,  1.03s/file]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Beauty Device (10) — already exists: stylekorean/data/product_urls/beautiful_device_&_tools_sub_categories/Beauty_Device__10_.json\n",
      "[SKIP] Cotton pad (3) — already exists: stylekorean/data/product_urls/beautiful_device_&_tools_sub_categories/Cotton_pad__3_.json\n",
      "[SKIP] Brush (41) — already exists: stylekorean/data/product_urls/beautiful_device_&_tools_sub_categories/Brush__41_.json\n",
      "[SKIP] Others (25) — already exists: stylekorean/data/product_urls/beautiful_device_&_tools_sub_categories/Others__25_.json\n",
      "[COMPLETE] Finished scraping: beautiful_device_&_tools_sub_categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subcategory Files: 100%|██████████| 5/5 [00:05<00:00,  1.03s/file]\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(subcat_data_dir)\n",
    "\n",
    "for file in tqdm(files, desc=\"Processing Subcategory Files\", unit=\"file\"):\n",
    "    base_filename = os.path.splitext(file)[0]\n",
    "    input_path = os.path.join(subcat_data_dir, file)\n",
    "    output_dir = os.path.join(product_data_root, base_filename)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    log_path = os.path.join(product_log_dir, f\"{base_filename}.log\")\n",
    "    logger = get_logger(base_filename, log_path)\n",
    "\n",
    "    tqdm.write(f\"Logging to: {log_path}\")\n",
    "\n",
    "    try:\n",
    "        sub_categories = read_subcategories_from_file(input_path)\n",
    "\n",
    "        # Count total targets first\n",
    "        all_targets = []\n",
    "        for sub_category in sub_categories:\n",
    "            children = sub_category['children'] if sub_category.get('children') else [sub_category]\n",
    "            all_targets.extend(children)\n",
    "\n",
    "        for entry in tqdm(all_targets, desc=f\"Scraping Products: {base_filename}\", unit=\"subcategory\", leave=False):\n",
    "            name, url = entry['name'], entry['url']\n",
    "            filename = sanitize_filename(name) + \".json\"\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                msg = f\"[SKIP] {name} — already exists: {output_path}\"\n",
    "                logger.info(msg)\n",
    "                tqdm.write(msg)\n",
    "                continue\n",
    "\n",
    "            msg = f\"[START] Scraping: {name} → {url}\"\n",
    "            logger.info(msg)\n",
    "            tqdm.write(msg)\n",
    "\n",
    "            try:\n",
    "                products = scrape_all_products_from_paginated_category(url, HEADER, delay=DELAY_BETWEEN_FILES)\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(products, f, ensure_ascii=False, indent=2)\n",
    "                msg = f\"[DONE] {name} — saved {len(products)} products to {filename}\"\n",
    "                logger.info(msg)\n",
    "                tqdm.write(msg)\n",
    "            except Exception as e:\n",
    "                msg = f\"[ERROR] {name} ({url}) — {str(e)}\"\n",
    "                logger.error(msg)\n",
    "                tqdm.write(msg)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[FATAL ERROR] Failed to process {file}: {str(e)}\")\n",
    "\n",
    "    tqdm.write(f\"[COMPLETE] Finished scraping: {base_filename}\")\n",
    "    time.sleep(DELAY_BETWEEN_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26164be5",
   "metadata": {},
   "source": [
    "# Get detail products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92deec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in tqdm(os.listdir(product_data_root), desc=\"Processing Folders\", unit=\"folder\"):\n",
    "    folder_path = os.path.join(product_data_root, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=f\"Processing Files in {folder}\", unit=\"file\", leave=False):\n",
    "        input_file = os.path.join(folder_path, filename)\n",
    "        urls_data = read_all_url_from_file(input_file)\n",
    "        if not urls_data:\n",
    "            continue\n",
    "\n",
    "        output_folder = os.path.join(product_detail_data_dir, folder)\n",
    "        ensure_dir(output_folder)\n",
    "\n",
    "        log_folder = os.path.join(product_detail_log_dir, folder)\n",
    "        ensure_dir(log_folder)\n",
    "\n",
    "        output_file = os.path.join(output_folder, filename)\n",
    "        log_file = os.path.join(log_folder, f\"{os.path.splitext(filename)[0]}.log\")\n",
    "\n",
    "        logger = get_logger(f\"{folder}_{filename}\", log_file)\n",
    "        logger.info(f\"Processing: {input_file}\")\n",
    "        tqdm.write(f\"Processing: {input_file}\")\n",
    "\n",
    "        # Load existing data\n",
    "        if os.path.exists(output_file):\n",
    "            with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                extracted_items = json.load(f)\n",
    "        else:\n",
    "            extracted_items = []\n",
    "\n",
    "        existing_urls = {item.get(\"original_url\") for item in extracted_items}\n",
    "        fetched_urls = load_fetched_urls_from_log(log_file)\n",
    "\n",
    "        for product in tqdm(urls_data, desc=f\"Fetching Products in {filename}\", unit=\"product\", leave=False):\n",
    "            name = product.get(\"name\")\n",
    "            url = product.get(\"url\")\n",
    "\n",
    "            if not url or url in fetched_urls or url in existing_urls:\n",
    "                msg = f\"[SKIP] Already fetched: {url}\"\n",
    "                logger.info(msg)\n",
    "                tqdm.write(msg)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                msg = f\"[FETCH] {url}\"\n",
    "                logger.info(msg)\n",
    "                tqdm.write(msg)\n",
    "\n",
    "                res = requests.get(url, timeout=10)\n",
    "                res.raise_for_status()\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "                    \n",
    "                data = extract_product_data(soup, DOMAIN)\n",
    "                data[\"original_name\"] = name\n",
    "                data[\"original_url\"] = url\n",
    "                data[\"category\"] = folder\n",
    "                data[\"subcategory\"] = os.path.splitext(filename)[0]\n",
    "\n",
    "\n",
    "                extracted_items.append(data)\n",
    "                logger.info(f\"Fetched: {url}\")\n",
    "\n",
    "                # Save incrementally\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(extracted_items, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                err_msg = f\"[ERROR] Fetching {url}: {e}\"\n",
    "                logger.error(err_msg)\n",
    "                tqdm.write(err_msg)\n",
    "\n",
    "            time.sleep(DELAY_BETWEEN_FILES)\n",
    "\n",
    "        logger.info(f\"[DONE] Saved final data: {output_file}\")\n",
    "        tqdm.write(f\"[DONE] Saved final data: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b5a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
